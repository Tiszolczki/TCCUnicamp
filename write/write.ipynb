{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c4c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "# Inicializa a SparkSession (idealmente fora do módulo em produção)\n",
    "spark = SparkSession.builder.appName(\"Data Export via Factory\").getOrCreate()\n",
    "\n",
    "# Interface de Exportação\n",
    "class DataExport(ABC):\n",
    "    @abstractmethod\n",
    "    def export(self, df: DataFrame, path: str, destination: str = \"local\") -> None:\n",
    "        \"\"\"Exporta um DataFrame para o caminho especificado, com base no destino.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Implementação concreta para exportar em formato Parquet para o sistema de arquivos local\n",
    "class ParquetExport(DataExport):\n",
    "    def export(self, df: DataFrame, path: str, destination: str = \"local\") -> None:\n",
    "        \"\"\"Exporta o DataFrame para o sistema de arquivos local ou S3, dependendo do destino.\"\"\"\n",
    "        if destination == \"local\":\n",
    "            self.export_local(df, path)\n",
    "        elif destination == \"s3\":\n",
    "            self.export_s3(df, path)\n",
    "        else:\n",
    "            raise ValueError(\"Destino inválido. Use 'local' ou 's3'.\")\n",
    "\n",
    "    def export_local(self, df: DataFrame, path: str) -> None:\n",
    "        \"\"\"Exporta o DataFrame para o sistema de arquivos local em formato Parquet.\"\"\"\n",
    "        df.write.mode(\"overwrite\").parquet(path)\n",
    "\n",
    "    def export_s3(self, df: DataFrame, path: str) -> None:\n",
    "        \"\"\"Exporta o DataFrame para o S3 em formato Parquet.\"\"\"\n",
    "        df.write.mode(\"overwrite\").parquet(f\"s3a://{path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
